apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: llama-finetune-job
  # namespace: default
spec:
  # RayJob will create a new RayCluster for this job and tear it down afterwards.
  # To use an existing RayCluster, you'd typically use `ray job submit` or Ray Client.
  # However, for a self-contained job, RayJob can define its own cluster.
  # For this example, let's assume we want to run on the *already deployed* `llama-raycluster`.
  # To do this, we need to set `submissionMode: K8sJobMode` and point to the cluster,
  # OR use Ray Client from within a submitter pod.
  # For simplicity with an existing cluster, `ray job submit` (Option A) is often easier.

  # If defining a cluster *within* the RayJob (it will be created and deleted with the job):
  # shutdownAfterJobFinishes: true
  # rayClusterSpec:
  #   <... include your RayCluster spec from Step 4 here, or a simplified one for the job ...>

  # For submitting to an *existing* RayCluster named 'llama-raycluster':
  # This typically requires the Ray Job Submission REST API or Ray Client.
  # A common pattern for RayJob CRD is to define the cluster *it will manage*.
  # If you want RayJob to submit to an *external, already running* RayCluster,
  # the entrypoint command itself would need to use `ray job submit` or Ray Client
  # targeting that external cluster's head service.

  # Simpler RayJob model: define the cluster spec here or use a long-running cluster + ray job submit.
  # Let's assume the RayJob creates its own cluster for this specific job run:
  shutdownAfterJobFinishes: true
  ttlSecondsAfterFinished: 600 # Optional: Time to live for job resources after completion

  entrypoint: python /home/ray/app/fine_tune_llama_ray.py # Path inside the Docker image

  # Define the cluster for this job
  rayClusterSpec:
    rayVersion: '2.20.0'
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: '0'
      template:
        spec:
          containers:
            - name: ray-head
              image: YOUR_IMAGE_NAME_HERE # From Step 3
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
              resources:
                limits: {cpu: "2", memory: "8Gi"}
                requests: {cpu: "1", memory: "4Gi"}
              # volumeMounts for HF token if needed
              # ...
          # volumes for HF token if needed
          # ...
    workerGroupSpecs:
    - replicas: 2
      minReplicas: 2
      maxReplicas: 2 # Fixed size for this job example
      groupName: gpu-workers
      rayStartParams: {}
      template:
        spec:
          nodeSelector:
            cloud.google.com/gke-accelerator: nvidia-l4
          tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
          containers:
          - name: ray-worker
            image: YOUR_IMAGE_NAME_HERE # From Step 3
            resources:
              limits: {cpu: "8", memory: "40Gi", "nvidia.com/gpu": "1"}
              requests: {cpu: "4", memory: "20Gi", "nvidia.com/gpu": "1"}
            env:
              - name: NVIDIA_VISIBLE_DEVICES
                value: all
              - name: NVIDIA_DRIVER_CAPABILITIES
                value: "compute,utility"
            # volumeMounts for HF token
            # ...
  
  # If your script is not part of the image, or you need to pass files:
  # runtimeEnvYAML: |
  #   working_dir: "gs://your-bucket/my-job-files/" # Example: fetch code from GCS
  #   pip:
  #     - "wandb"